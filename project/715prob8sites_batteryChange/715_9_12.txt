PRISM
=====

Version: 4.4.beta
Date: Tue Dec 12 18:46:11 EST 2017
Hostname: klyu
Memory limits: cudd=1g, java(heap)=910.5m
Command line: prism Project_Code2.pm -pctl 'multi(R{"totalRewards"}max=? [C], R{"negatives"}min=? [C])' -exportadv policy.txt -s -exportstates states.txt -s

Parsing model file "Project_Code2.pm"...

1 property:
(1) multi(R{"totalRewards"}max=? [ C ], R{"negatives"}min=? [ C ])

Type:        MDP
Modules:     UAV site_one site_two site_three site_four site_five site_six site_seven site_eight site_nine 
Variables:   uav uav_battery s1 s2 s3 s4 s5 s6 s7 s8 s9 

Building model...

Computing reachable states...

Reachability (BFS): 21 iterations in 0.03 seconds (average 0.001524, setup 0.00)

Time for model construction: 0.074 seconds.

Warning: Deadlocks detected and fixed in 14080 states

Type:        MDP
States:      92160 (1 initial)
Transitions: 2252032
Choices:     1053952

Transition matrix: 1505 nodes (4 terminal), 2252032 minterms, vars: 18r/18c/14nd

Exporting list of reachable states in plain text format to file "states.txt"...

---------------------------------------------------------------------

Model checking: multi(R{"totalRewards"}max=? [ C ], R{"negatives"}min=? [ C ])

Warning: Disabling Prob1 since this is needed for adversary generation
Total time for product construction: 0.0 seconds.

States:      92160 (1 initial)
Transitions: 2252032
Choices:     1053952

Transition matrix: 1505 nodes (4 terminal), 2252032 minterms, vars: 18r/18c/14nd

Prob0A: 1 iterations in 0.00 seconds (average 0.000000, setup 0.00)

yes = 0, no = 0, maybe = 92160

Computing remaining probabilities...
Engine: Sparse
Optimising weighted sum for reward objective 1/2: weights (1.0, 0.0)
Iterative method: 23 iterations in 0.82 seconds (average 0.035478, setup 0.00)
Optimal value for weights [1.000000,0.000000] from initial state: 450.000000
Computed point: (450.0, -1.0263809110937496)
Optimising weighted sum for reward objective 2/2: weights (0.0, 1.0)
Iterative method: 19 iterations in 0.63 seconds (average 0.033263, setup 0.00)
Optimal value for weights [0.000000,1.000000] from initial state: 0.000000
Computed point: (289.38388127812493, 0.0)
Optimising weighted sum of objectives: weights (0.0063496970996135705, 0.9936503029003865)
Iterative method: 22 iterations in 0.81 seconds (average 0.036909, setup 0.00)
Optimal value for weights [0.006350,0.993650] from initial state: 1.922022

Adversary written to file "policy1.txt".
Optimising weighted sum of objectives: weights (0.0050657438188032545, 0.9949342561811968)
Iterative method: 22 iterations in 0.82 seconds (average 0.037455, setup 0.00)
Optimal value for weights [0.005066,0.994934] from initial state: 1.470328

Adversary written to file "policy2.txt".
Optimising weighted sum of objectives: weights (0.007231848497892659, 0.9927681515021073)
Iterative method: 22 iterations in 0.82 seconds (average 0.037273, setup 0.00)
Optimal value for weights [0.007232,0.992768] from initial state: 2.291062

Adversary written to file "policy3.txt".
Optimising weighted sum of objectives: weights (0.006557553772353274, 0.9934424462276468)
Iterative method: 22 iterations in 0.82 seconds (average 0.037091, setup 0.00)
Optimal value for weights [0.006558,0.993442] from initial state: 2.006456

Adversary written to file "policy4.txt".
Optimising weighted sum of objectives: weights (0.01144237813178733, 0.9885576218682127)
Iterative method: 22 iterations in 0.83 seconds (average 0.037636, setup 0.00)
Optimal value for weights [0.011442,0.988558] from initial state: 4.158230

Adversary written to file "policy5.txt".
Optimising weighted sum of objectives: weights (0.0063552715984898376, 0.9936447284015102)
Iterative method: 22 iterations in 0.98 seconds (average 0.044364, setup 0.00)
Optimal value for weights [0.006355,0.993645] from initial state: 1.924002

Adversary written to file "policy6.txt".
Optimising weighted sum of objectives: weights (0.006908782471002711, 0.9930912175289973)
Iterative method: 22 iterations in 0.82 seconds (average 0.037273, setup 0.00)
Optimal value for weights [0.006909,0.993091] from initial state: 2.151117

Adversary written to file "policy7.txt".
Optimising weighted sum of objectives: weights (0.00951794811453647, 0.9904820518854636)
Iterative method: 22 iterations in 0.82 seconds (average 0.036909, setup 0.00)
Optimal value for weights [0.009518,0.990482] from initial state: 3.294030

Adversary written to file "policy8.txt".
Optimising weighted sum of objectives: weights (0.03909061331726427, 0.9609093866827357)
Iterative method: 23 iterations in 0.90 seconds (average 0.039130, setup 0.00)
Optimal value for weights [0.039091,0.960909] from initial state: 16.618062

Adversary written to file "policy9.txt".
Optimising weighted sum of objectives: weights (0.0194096553198414, 0.9805903446801586)
Iterative method: 22 iterations in 0.84 seconds (average 0.038182, setup 0.00)
Optimal value for weights [0.019410,0.980590] from initial state: 7.745693

Adversary written to file "policy10.txt".
Optimising weighted sum of objectives: weights (0.11762870138337078, 0.8823712986166292)
Iterative method: 23 iterations in 0.88 seconds (average 0.038435, setup 0.00)
Optimal value for weights [0.117629,0.882371] from initial state: 52.027270

Adversary written to file "policy11.txt".
The value iteration(s) took 11.191 seconds altogether.
Number of weight vectors used: 13
Multi-objective value iterations took 11.191 s.

Value in the initial state: [(312.4928119968749, 0.11325374999999997), (289.38388127812493, -0.0), (354.87970797480466, 0.333474375), (406.95216683378897, 0.666525625), (412.13722152128895, 0.7010926562499998), (436.92540824003896, 0.8750448437499998), (447.1468074685546, 0.9711332031249997), (449.1729719490234, 0.9927367046875), (450.0, 1.0263809110937496), (449.8478262650389, 1.0060946678124998), (449.5416858474608, 0.9991491203124999)]

Time for model checking: 25.686 seconds.

Result: [(312.4928119968749, 0.11325374999999997), (289.38388127812493, -0.0), (354.87970797480466, 0.333474375), (406.95216683378897, 0.666525625), (412.13722152128895, 0.7010926562499998), (436.92540824003896, 0.8750448437499998), (447.1468074685546, 0.9711332031249997), (449.1729719490234, 0.9927367046875), (450.0, 1.0263809110937496), (449.8478262650389, 1.0060946678124998), (449.5416858474608, 0.9991491203124999)] (value in the initial state)

---------------------------------------------------------------------

Note: There were 2 warnings during computation.

