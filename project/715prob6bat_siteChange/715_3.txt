PRISM
=====

Version: 4.4.beta
Date: Tue Dec 12 18:33:14 EST 2017
Hostname: klyu
Memory limits: cudd=1g, java(heap)=910.5m
Command line: prism Project_Code2.pm -pctl 'multi(R{"totalRewards"}max=? [C], R{"negatives"}min=? [C])' -exportadv policy.txt -s -exportstates states.txt -s

Parsing model file "Project_Code2.pm"...

1 property:
(1) multi(R{"totalRewards"}max=? [ C ], R{"negatives"}min=? [ C ])

Type:        MDP
Modules:     UAV site_one site_two site_three 
Variables:   uav uav_battery s1 s2 s3 

Building model...

Computing reachable states...

Reachability (BFS): 10 iterations in 0.00 seconds (average 0.000000, setup 0.00)

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 100 states

Type:        MDP
States:      384 (1 initial)
Transitions: 2932
Choices:     1588

Transition matrix: 435 nodes (4 terminal), 2932 minterms, vars: 10r/10c/7nd

Exporting list of reachable states in plain text format to file "states.txt"...

---------------------------------------------------------------------

Model checking: multi(R{"totalRewards"}max=? [ C ], R{"negatives"}min=? [ C ])

Warning: Disabling Prob1 since this is needed for adversary generation
Total time for product construction: 0.0 seconds.

States:      384 (1 initial)
Transitions: 2932
Choices:     1588

Transition matrix: 435 nodes (4 terminal), 2932 minterms, vars: 10r/10c/7nd

Prob0A: 1 iterations in 0.00 seconds (average 0.000000, setup 0.00)

yes = 0, no = 0, maybe = 384

Computing remaining probabilities...
Engine: Sparse
Optimising weighted sum for reward objective 1/2: weights (1.0, 0.0)
Iterative method: 10 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [1.000000,0.000000] from initial state: 150.000000
Computed point: (150.0, -1.0)
Optimising weighted sum for reward objective 2/2: weights (0.0, 1.0)
Iterative method: 7 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [0.000000,1.000000] from initial state: 0.000000
Computed point: (102.53125, 0.0)
Optimising weighted sum of objectives: weights (0.020631850419084462, 0.9793681495809154)
Iterative method: 10 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [0.020632,0.979368] from initial state: 2.155492

Adversary written to file "policy1.txt".
Optimising weighted sum of objectives: weights (0.019607843137254898, 0.9803921568627451)
Iterative method: 9 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [0.019608,0.980392] from initial state: 2.010417

Adversary written to file "policy2.txt".
Optimising weighted sum of objectives: weights (0.024929634097305996, 0.975070365902694)
Iterative method: 10 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [0.024930,0.975070] from initial state: 2.782107

Adversary written to file "policy3.txt".
Optimising weighted sum of objectives: weights (0.022988505747126416, 0.9770114942528736)
Iterative method: 10 iterations in 0.00 seconds (average 0.000000, setup 0.00)
Optimal value for weights [0.022989,0.977011] from initial state: 2.489368

Adversary written to file "policy4.txt".
Optimising weighted sum of objectives: weights (0.11764705882353621, 0.8823529411764638)
Iterative method: 10 iterations in 0.01 seconds (average 0.000800, setup 0.00)
Optimal value for weights [0.117647,0.882353] from initial state: 16.764706

Adversary written to file "policy5.txt".
The value iteration(s) took 0.009 seconds altogether.
Number of weight vectors used: 7
Multi-objective value iterations took 0.009 s.

Value in the initial state: [(140.90625, 0.7675), (102.53125, -0.0), (149.83125, 0.9775), (150.0, 1.0)]

Time for model checking: 0.052 seconds.

Result: [(140.90625, 0.7675), (102.53125, -0.0), (149.83125, 0.9775), (150.0, 1.0)] (value in the initial state)

---------------------------------------------------------------------

Note: There were 2 warnings during computation.

